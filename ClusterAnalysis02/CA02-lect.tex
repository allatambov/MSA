\documentclass[12pt,a4paper]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{geometry}

\geometry{
left=15mm,
top=20mm,
right = 20mm,
bottom = 25mm
}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{enumitem}
\setlength{\parindent}{0cm}
\setlength{\parskip}{6pt}

\usepackage{titlesec}
\titleformat{\section}[block]{\large\bfseries}{\S\thesection.}{0.5em}{}

\theoremstyle{definition}
\newtheorem{example}{Пример}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{width=6cm,compat=1.12}

\begin{document}
\textbf{Введение в многомерный статистический анализ\\
2021-2022 учебный год}\medskip\\
\textit{Автор лекции: Тамбовцева А.А.}

\section*{Лекция 2. Кластерный анализ методом k-средних}
\setcounter{section}{1}

\subsection{Алгоритм кластерного анализа} 

Общий алгоритм кластерного анализа можно сформулировать 
в виде следующих шагов:

\begin{enumerate}
\item Реализовать иерархический кластерный анализ.
\item По итогам иерархического кластерного анализа выбрать число
кластеров $k$, исходя из содержательных соображений и более 
формальных методов. Примеры методов:

\begin{itemize}
\item метод согнутого локтя или согнутого колена 
(\textit{Elbow method});
\item силуэтный метод (\textit{Silhouette method}).
\end{itemize}

\item Проверить с помощью формальных методов, насколько различия 
между группами существенны.

\item Реализовать более точный кластерный анализ с помощью метода 
k-средних (\textit{k-means}) с выбранным числом групп $k$.

\item Сравнить результаты разных реализаций кластерного анализа 
в качестве проверки устойчивости, стабильности результатов.
\end{enumerate}

\subsection{Кластеризация методом k-means} 

Общая идея метода k-means (k-средних):

\begin{itemize}
\item На входе p-мерный массив, число кластеров $k$.
\item На выходе хотим получить такое деление на кластеры, при котором 
внутригрупповой разброс минимален.
\end{itemize}

Остановимся на идее минимизации поподробнее. Для начала зафиксируем 
обозначения для кластеров и наблюдений.

Пусть $C_1, C_2, \dots, C_k$ -- наборы (множества) наблюдений в каждом 
кластере, обладающие следующими свойствами:

\begin{enumerate}
\item $C_1 \cup C_2 \cup C_k = \{1,2,\dots, n\}$ (все наблюдения распределены по кластерам);
\item $C_i \cap C_j \ne \emptyset$ для всех $i\ne j$ (никакие два кластера не пересекаются).
 \end{enumerate}

Пользуясь введёнными обозначениями, определим
функцию внутригруппового разброса для кластера $k$:

$$
W(C_k)=\dfrac{1}{|C_k|}  \sum_{i, i' \in C_k} \sum^p_{j=1}(x_{ij}-x_{i'j})^2,
$$

где $|C_k|$ – мощность множества $C_k$, то есть количество наблюдений 
в кластере $k$. 

Содержательно, эта функция – просто мера среднего расстояния между точками 
кластера $k$: мы вычисляем квадраты евклидовых расстояний между всеми 
парами точек $i$ и $i'$ с учётом всех $p$ измерений, суммируем результаты и 
делим на количество элементов.

Как будет выглядеть мера общего внутригруппового разброса? 
Очень просто – это сумма внутригрупповых разбросов всех 
кластеров от 1 до $K$:

$$
W(C_1, \dots, C_k) = \sum^K_{k=1} W(C_k).
$$

Теперь задача метода k-means сводится к тому, чтобы подобрать 
такое разбиение на кластеры $C_1, C_2, \dots, C_k$, чтобы этот 
суммарный внутригрупповой разброс минимизировать:

$$
\sum^k_{k=1} W(C_k) \xrightarrow[C_1...C_K]{} \min.
$$

Решить эту оптимизационную задачу точно довольно сложно, высока 
вычислительная сложность – всего существует $K^n$ способов разбить 
$n$ наблюдений на $K$ групп. Поэтому на практике используется 
приближённый алгоритм нахождения разбиения на кластеры, который 
включает элемент случайности.

Приближённый алгоритм нахождения разбиения на кластеры методом k-means:

\begin{enumerate}
\item Cлучайным образом закрепляем за наблюдениями метки 
кластеров -- числа от $1$ до $K$. 
\item Повторяем до тех пор, пока не~получим наилучшее возможное качество -- пока 
метки кластеров не перестанут меняться -- следующие шаги:

\begin{itemize}
\item определить центроид кластера;
\item приписать точку к~кластеру, расстояние до центроида 
которого минимальное из всех возможных. 
\end{itemize}
\end{enumerate}

Приведённый выше алгоритм является эффективным и 
даёт хорошие результаты, однако стоит помнить, что, во-первых, 
он несёт в себе элемент случайности, а во-вторых, находит не глобальный, 
а локальный минимум функции $\sum^k_{k=1} W(C_k)$. Поэтому 
разумным решением будет запускать кластеризацию методом k-means 
несколько раз, сравнивать результаты и выбирать тот вариант, который лучшим 
образом подходит по содержательным соображениям и итогам 
предварительного иерархического кластерного анализа.


\end{document}
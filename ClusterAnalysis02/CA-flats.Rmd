---
title: "Иерархический кластерный анализ и метод k-means"
author: "Алла Тамбовцева"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
mainfont: CMU Serif
header-includes:
- \usepackage[russian]{babel}
- \usepackage{hyperref}
- \hypersetup{colorlinks = true, urlcolor = blue, linkcolor=blue}
---

### Установка библиотек и загрузка данных

Установим необходимые библиотеки (считаем, что `tidyverse`, которая включает в себя `ggplot2` для графиков уже установлена):

```{r, eval=FALSE}
install.packages("factoextra")
install.packages("NbClust")
install.packages("fossil")
```

Загрузим данные по ценам на квартиры в Москве из файла `flats.csv` и удалим строки с пропущенными значениями (их нет, но оставим код для универсальности):

```{r, include=FALSE}
flats <- read.csv("/Users/allat/Desktop/flats.csv")
flats <- na.omit(flats)
```

```{r, eval=FALSE}
flats <- read.csv(file.choose())
flats <- na.omit(flats)
```

Переменные в файле:

* `price`: цена квартиры, в 1000$;
* `totsp`: общая площадь, в кв.метрах;
* `livesp`: жилая площадь, в кв.метрах;
* `kitsp`: площадь кухни, в кв.метрах;
* `dist`: расстояние до центра города, в км;
* `metrdist`: расстояние до метро, в мин;
* `walk`: шаговая доступность до метро, 0 или 1;
* `brick`: дом из кирпича или аналогичного «капитального» материала, 0 или 1; 
* `floor`: удобный ли этаж (не первый и последний), 0 или 1.

Посмотрим на переменные и проверим, что все переменные корректных типов (например, числа считаны как числа, а не как текст):

```{r}
str(flats)
```
Примечание: тип `int` — целочисленный (`integer`), тип `num` — просто числовой (`numeric`), может включать в себя как дробные, так и целочисленные значения.

### Описание данных

Посмотрим на описательные статистики по всем столбцам:

```{r}
summary(flats)
```

С описательными статистиками все знакомы, отметим здесь только то, что в бинарных показателях среднее корректно интерпретировать как долю «единиц», то есть мы можем определить выборочную долю квартир в шаговой доступности от метро и проч.

Загрузим библиотеку `tidyverse` и визуализируем количественные показатели.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
```

---------------------------------------
**Пояснения к коду для графиков с `ggplot()`**
---------------------------------------

В аргумент `data` записываем название датафрейма, внутри `aes()` указываем, какие переменные идут по осям `x` и `y`. Далее через `+` добавляем слои, отвечающие за тип и внешний вид графика:

— `geom_histogram()`: гистограмма, фиксируем цвет заливки `fill` и цвет границ столбцов `color`;

— `theme_bw()`: чёрно-белая тема для фона и разметки;

— `labs()`: заголовки и подписи к осям.

---

Построим гистограмму для цен на квартиры:

```{r, message=FALSE, warning=FALSE, fig.height=3}
ggplot(data = flats, aes(x = price)) + 
  geom_histogram(fill = "cornflowerblue", color = "black") + 
  theme_bw() +
  labs(x = "Price (in 1000 $)")
```

Гистограмма для площади квартир:

```{r, message=FALSE, warning=FALSE, fig.height=3}
ggplot(data = flats, aes(x = totsp)) + 
  geom_histogram(fill = "firebrick", color = "black") + 
  theme_bw() +
  labs(x = "Total square (in meters sq)")
```

Гистограмма для расстояния до центра:

```{r, message=FALSE, warning=FALSE, fig.height=3}
ggplot(data = flats, aes(x = dist)) + 
  geom_histogram(binwidth = 1.5, fill = "limegreen", color = "black") + 
  theme_bw() + 
  labs(x = "Distance to the city center (in km)")
```

Гистограмма для расстояния до метро:

```{r, message=FALSE, warning=FALSE, fig.height=3}
ggplot(data = flats, aes(x = metrdist)) + 
  geom_histogram(binwidth = 2, fill = "hotpink", color = "black") + 
  theme_bw() + 
  labs(x = "Distance to the metro station (in min)")
```

### Иерархический кластерный анализ

Теперь перейдём к иерархическому кластерному анализу. Сформируем матрицу расстояний через `dist()`, предварительно прошкалировав наши данные с помощью `scale()`:

```{r}
D <- dist(scale(flats))
```

По умолчанию функция `dist` вычисляет евклидовы расстояния, они нас устраивают. В качестве метода агрегирования возьмём метод Уорда как наиболее эффективный, но учтём, что этот метод на входе требует квадраты евклидовых расстояний (опция `ward.D2`, не просто `ward.D`):

```{r}
hc_ward <- hclust(D, method = "ward.D2")
```

Построим дендрограмму (она будет довольно дикая из-за большого числа наблюдений, но нам её хватит для определения крупных кластеров):

```{r}
plot(hc_ward)
```

Если мы посмотрим внимательно на дендрограмму, мы сможем выделить разное число кластеров. Выбор в данном случае зависит от того, насколько общее мы хотим получить деление (на мой взгляд, остановиться на двух группах здесь было бы слишком просто), и от содержательных соображений. 

Выберем три кластера и «разрежем» дендрограмму на высоте *Height*, которое соответствует делению на три группы. R сам его посчитает с помощью функции `cutree()` и припишет наблюдениями, относящимся к разным кластерам, соответствующие метки:

```{r}
ward <- cutree(hc_ward, k = 3)
```

Сделаем эти метки факторными (качественными) и добавим их в исходный датафрейм:

```{r}
flats$ward <- factor(ward)
```

### Содержательная интерпретация кластеров

Оценим, что получилось. Посмотрим на описательные статистики по группам. 

---------------------------------------
**Пояснения к коду для описания групп**
---------------------------------------

Функция `summarise_at()` позволяет применить функции для описания или агрегирования данных, указанные внутри `.funs()` к фиксированному набору переменных, которые находятся внутри `vars()`. 

Здесь взяты переменные с `price` до `floor`, и применяем мы функции `median()` и `mean()`. Можно было бы применить их одновременно, записать несколько функций в виде вектора, но тогда выдача была бы менее удобной, плюс, всё равно состояла бы из двух частей. 

---

Сначала посмотрим на медианные значения показателей:

```{r}
flats %>% group_by(ward) %>% summarise_at(vars(price:floor), 
                                             .funs = median)
```
Что мы здесь видим? Однозначно одно: в первом кластере собраны самые дорогие и самые большие по площади квартиры. Про остальные два кластера пока всё не так ясно. Поэтому теперь посмотрим на средние значения:

```{r}
flats %>% group_by(ward) %>% summarise_at(vars(price:floor), 
                                             .funs = mean)
```

Вот здесь уже всплывает очень интересный факт: так как для бинарных показателей среднее совпадает с долей «единиц», получается, что во втором кластере 100% квартир находятся на неудобных этажах, а в третьем — на удобных (и да, квартиры во втором кластере логичным образом, в среднем, дешевле).

Вернёмся к цене и визуализируем различия в её распределении по группам.

---------------------------------------
**Пояснения к коду для графиков с `ggplot()`**
---------------------------------------

В аргумент `data` записываем название датафрейма, внутри `aes()` указываем, какие переменные идут по осям `x` и `y`. В данном случае по оси `x` указаны группы по результатам кластерного анализа, а по оси `y` – цена (классический ящик с усами – вертикальный график). Кроме того, внутри `aes()` мы зафиксировали, что цвет заливки ящиков должен различаться в зависимости от группы.

Далее добавляем слои, отвечающие за тип и внешний вид графика:

— `geom_boxplot()`: ящик с усами;

— `theme_bw()`: чёрно-белая тема для фона и разметки;

— `labs()`: заголовки и подписи к осям.

---------------------------------------

```{r, message=FALSE, warning=FALSE, fig.height=3}
ggplot(data = flats, aes(x = ward, y = price, fill = ward)) + 
  geom_boxplot() + 
  theme_bw() + 
  labs(x = "Price (in 1000 $)")
```

Разница в цене квартир во второй и третьей группе не очень большая, но это не означает, что кластеризация у нас получилась плохая. Мы изучаем методы многомерного анализа данных, и сами группы свободно могут отличаться друг от друга по другим измерениям. 

Попробуем посмотреть, образуют ли точки, относящиеся к разным кластерам, группы на диаграмме рассеивания.

---------------------------------------
**Пояснения к коду для графиков с `ggplot()`**
---------------------------------------

Также как и ранее, добавляем в `aes()` указание на то, что цвет точек зависит от группы в столбце `ward`. Плюс, чтобы сократить разброс точек на графике (есть нетипичные значения, супер-дорогая огромная квартира), добавляем слои `scale_x_log10()` и `scale_y_log10()`, чтобы по горизонтальной и вертикальной осям указывались логарифмированные показатели вместо исходных. 

---------------------------------------

```{r, message=FALSE, warning=FALSE, fig.height=2}
ggplot(data = flats, aes(x = totsp, y = price, color = ward)) + 
  geom_point() + theme_bw() + 
  scale_x_log10() + scale_y_log10()
```

Опять видим такую картину: первый кластер явно выделяется (большие и дорогие квартиры), два других перемешаны с точки зрения цены и площади. 

\newpage
---------------------------------------
**Небольшое отступление по заявкам слушателей про трёхмерные графики.**
---------------------------------------

По-хорошему, чтобы действительно показать различия между вторым и третьим кластером на диаграмме рассеивания такого вида, нам понадобится трёхмерный график. Установим библиотеку `rgl`:

```{r, eval=FALSE}
install.packages("rgl")
```

Теперь построим сам график. Строки `setupKnitr()` и `rglwidget()` нужны для отображения графика HTML/Word/PDF при связывании файла RMarkdown, сам по себе он открывается в новом интерактивном окне. В Word/PDF график будет представлен в статичном виде, а в HTML сохранится интерактив, график можно будет поворачивать, увеличивать и прочее.

Сначала зафиксируем цвета точек и сохраним их в отдельный столбец датафрейма (это обычная операция для графиков за рамками `ggplot2`).

```{r}
mycolors <- c('#F8766D', '#00BA38', '#619CFF')
flats$color <- mycolors[as.numeric(flats$ward)]
```

Выбранные цвета в `mycolors` — это цвета, которые по умолчанию использовались `ggplot()` на диаграмме выше, записанные в формате HEX. 

Наконец, переходим к графику (`type` - тип точек *sphere*, далее фиксируем радиус этих сфер):

```{r, message=FALSE, warning=FALSE, fig.height=3}
library(rgl)

setupKnitr()

plot3d( 
  x = flats$totsp, 
  y = flats$price, 
  z = flats$floor, 
  col = flats$color, 
  type = 's', 
  radius = 8,
  xlab="Total space", 
  ylab="Price (in 1000$)", 
  zlab="Floor")

rglwidget()
```

Вот теперь точно видно, что у нас есть два «висящих» облака точек, в верхнем облаке, соответствующем квартирам на удобных этажах, находятся все наблюдения из третьего кластера, в нижнем — наблюдения из второго кластера. Ну, а наблюдения из первого кластера встречаются и там, и там (довольно жизненно: дорогие квартиры можно встреить на любом этаже).

---------------------------------------

### Выбор количества кластеров

Рассмотрим два основных метода: метод согнутого локтя (*Elbow method*) и силуэтный метод (*Silhouette method*).

---------------------------------------
**Пояснения к коду для графиков с `fviz_nbclust()`**
---------------------------------------

Для обоих методов мы используем функцию `fviz_nbclust()` из библиотеки 
`factoextra`. Так как метки кластеров из столбца `ward` для кластеризации нам не нужны, а нужны только исходные данные, из датафрейма `flats` мы выбираем столбцы с первого по девятый. 

Чтобы выбрать количество кластеров, нам нужно оценить качество кластеризации при каждом возможном количестве кластеров (в разумных пределах, количество групп не более 10). Выбираем метод кластеризации `kmeans`, он является более точным и требует на входе количество кластеров (будет изменяться от 1 до 10).

Для метода согнутого локтя выбираем меру разброса `wss`, общую внутригрупповую сумму квадратов отклонений, так как логика использования этого метода предполагает работу с различиями внутри групп.

Для силуэтного метода логичным образом выбираем `silhouette`. Силуэт — мера сходства наблюдений в кластере. Подробнее про силуэты и их вычисление можно почитать [здесь](https://en.wikipedia.org/wiki/Silhouette_(clustering)).

---------------------------------------

Метод согнутого локтя:

```{r, message=FALSE, warning=FALSE, fig.height=3}
library(factoextra)
fviz_nbclust(flats[1:9], kmeans, method = "wss") +
  labs(subtitle = "Elbow method")
```

Как следует из названия этого метода, на графике мы должны найти «локоть» (или «колено»), то есть такое число кластеров, начиная с которого общий внутригрупповой разброс начинает незначительно уменьшаться. Другими словами, мы стараемся подобрать такое число групп, чтобы наблюдения внутри групп были минимально отдалены друг от друга и чтобы при этом число этих групп не было избыточно большим (зачем дробить наблюдения на большее число групп, если схожесть наблюдений внутри при этом несильно увеличивается).

Выбранное нами ранее количество кластеров $k=3$ вполне согласуется с этим графиком. Слишком общее деление на две группы нам неинтересно, а «локоть» здесь находится в районе $k=3$ и $k=4$. Можете проверить самостоятельно, что при выделении четырёх кластеров они будут менее интерпретируемыми.

Силуэтный метод:

```{r, fig.height=3}
fviz_nbclust(flats[1:9], kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
```

На этом графике пик наблюдается при количестве кластеров 2, но мы снова пойдём дальше и выберем значение $k=3$, при котором происходит следующий излом. После этого излома средняя ширина силуэта (специфическая мера различия наблюдений в группе) убывает несильно и довольно равномерно, то есть, при делении на большее число групп эти группы не будут становиться более стабильными с точки зрения разброса значений.

В R также доступен менее классический, не очень устойчивый, способ выявления числа кластеров. Он доступен в библиотеке `NbClust` и делает следующее: запускает кластеризацию методом `kmeans` с разным числом кластеров (сами выбираем минимальное и максимальное), оценивает качество полученной кластеризации с помощью разных алгоритмов и сообщает количество кластеров, за которое «проголосовало» большинство используемых алгоритмов. 

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# takes time, not executed in this file 

library(NbClust)
res <- NbClust(flats[1:9], 
               min.nc = 2, max.nc = 8, 
               method = "kmeans")
fviz_nbclust(res)
```

### Оценка качества кластеризации

Проверим с помощью классических статистических методов, есть ли различия между полученными нами тремя кластерами.

Например, проверим, есть ли разница в медианной цене на квартиры в трёх кластерах, с помощью критерия Краскелла-Уолииса (нормальности распределения цены не наблюдается):

```{r}
kruskal.test(flats$price ~ flats$ward)
```
P-value примерно 0, отвергаем гипотезу об отсутствии различий медиан/распределений, следовательно, распределение цен на квартиры отличается в трёх кластерах.

Теперь поработаем с качественными показателями `walk`, `brick`, `floor`. Сравнивать доли «единиц» попарно в каждой паре групп (1 vs 2, 1 vs 3, 2 vs 3) неэффективно, поэтому просто перейдём к таблицам сопряжённости и критерию хи-квадрат. 

```{r}
tab_walk <- table(flats$ward, flats$walk)
tab_walk
chisq.test(tab_walk)
```

```{r}
tab_brick <- table(flats$ward, flats$brick)
tab_brick
chisq.test(tab_brick)
```

```{r}
tab_floor <- table(flats$ward, flats$floor)
tab_floor
chisq.test(tab_floor)
```
В целом, на 5%-ном (в первом случае на 10%-ном) уровне значимости мы можем считать, что есть связь между нашим делением квартир на группы и качественными характеристиками этих квартир, таких как шаговая доступность метро, материал дома, удоство этажа.

### Кластеризация методом k-средних

Проведём кластеризацию методом k-средних, этот метод считается более точным, плюс, теперь с количеством групп мы точно определились, его использование стало возможным. Метки кластеров по итогам реализации алгоритма также сохраним в датафрейм в факторном виде:

```{r}
kclust <- kmeans(flats[1:9], 3)
flats$k <- factor(kclust$cluster)  
```

### Сравнение различных реализаций кластерного анализа

Сравним деление на группы, полученное методом Уорда и методом k-means:

```{r}
flats %>% group_by(ward) %>% summarise_at(vars(price:floor), 
                                          .funs = c(mean))

flats %>% group_by(k) %>% summarise_at(vars(price:floor), 
                                          .funs = c(mean))
```

В целом, основания для кластеризации в двух методах похожи (главную роль играет цена и площадь), но решать, какая из кластеризаций нам подходит больше, стоит на содержательном уровне. С одной стороны, метод k-means более точный, с другой стороны, решение, предлагаемое методом Уорда, более понятное и интерпретируемое, если вспомнить чёткое деление квартир по удобству этажей.

Если хочется получить более формальный результат сравнения делений на группы, можно воспользоваться индексом Ранда (*Rand's index*). Он показывает, какая доля наблюдений группируется одинаково в двух вариантах кластеризации (его вычисление довольно интуитивное, идёт сравнение двух множеств с метками кластеров, можно почитать [здесь](https://en.wikipedia.org/wiki/Rand_index#:~:text=The%20Rand%20index%20or%20Rand,is%20the%20adjusted%20Rand%20index.)).

```{r, message=FALSE, warning=FALSE}
library(fossil)

rand.index(as.integer(flats$ward), 
           as.integer(flats$k))
```

Обратите внимание: функция `rand.index()` принимает на вход целочисленные метки кластеров.

Сходство не очень высокое, но это ожидаемо: мы уже выяснили, что в методе Уорда одним из ключевых оснований для деления на группы был этаж, а в кластеризации методом k-means — цена. Тем не менее, с учётом различий в логике используемых алгоритмов, сходство более 50% можно считать довольно высоким, поэтому заключаем, что деление на три кластера вышло удачным, как с содержательной, так и со статистической точки зрения.

